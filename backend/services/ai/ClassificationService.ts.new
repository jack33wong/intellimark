import { ImageAnnotatorClient } from '@google-cloud/vision';
import { ModelProvider } from './ModelProvider.js';
import type { ModelType, ImageClassification } from '../../types/index.js';

export interface TextAnalysisResult {
  mode: 'question' | 'marking';
  questionText: string;
  confidence: number;
  reasoning: string;
  usageTokens?: number;
}

export interface VisionResult {
  passA: any[];
  passB: any[];
  passC: any[];
  allBlocks: any[];
  passAText: string;
}

export class ClassificationService {
  private static readonly RESIZE_FACTOR = 2;

  /**
   * Perform Google Vision text extraction with 3-pass approach
   */
  private static async performRobustVisionExtraction(imageBuffer: Buffer): Promise<VisionResult> {
    const client = new ImageAnnotatorClient();
    const allBlocks: any[] = [];

    // Pass A: Clean Scan for Completeness
    let passABlocks: any[] = [];
    try {
      const [resultA] = await client.textDetection(imageBuffer);
      passABlocks = this.processTextAnnotation(resultA.fullTextAnnotation, 'pass_A_clean_scan');
      allBlocks.push(...passABlocks);
    } catch (error) {
      console.error('‚ùå [GOOGLE VISION] Pass A failed:', error);
    }

    // Pass B: Enhanced Scan for Accuracy
    let passBBlocks: any[] = [];
    try {
      const sharp = await import('sharp');
      const originalMetadata = await sharp.default(imageBuffer).metadata();
      const preprocessedBufferB = await sharp.default(imageBuffer)
        .resize((originalMetadata.width || 0) * this.RESIZE_FACTOR)
        .grayscale()
        .normalize()
        .toBuffer();
      const [resultB] = await client.textDetection(preprocessedBufferB);
      passBBlocks = this.processTextAnnotation(resultB.fullTextAnnotation, 'pass_B_enhanced_scan', this.RESIZE_FACTOR);
      allBlocks.push(...passBBlocks);
    } catch (error) {
      console.error('‚ùå [GOOGLE VISION] Pass B failed:', error);
    }

    // Pass C: Aggressive Scan for Edge Cases
    let passCBlocks: any[] = [];
    try {
      const sharp = await import('sharp');
      const originalMetadata = await sharp.default(imageBuffer).metadata();
      const preprocessedBufferC = await sharp.default(imageBuffer)
        .resize((originalMetadata.width || 0) * this.RESIZE_FACTOR)
        .sharpen()
        .threshold()
        .toBuffer();
      const [resultC] = await client.textDetection(preprocessedBufferC);
      passCBlocks = this.processTextAnnotation(resultC.fullTextAnnotation, 'pass_C_aggressive_scan', this.RESIZE_FACTOR);
      allBlocks.push(...passCBlocks);
    } catch (error) {
      console.error('‚ùå [GOOGLE VISION] Pass C failed:', error);
    }

    const passAText = passABlocks.map(block => block.text).join('\n');

    return {
      passA: passABlocks,
      passB: passBBlocks,
      passC: passCBlocks,
      allBlocks,
      passAText
    };
  }

  /**
   * Process text annotation from Google Vision
   */
  private static processTextAnnotation(fullTextAnnotation: any, source: string, scale: number = 1): any[] {
    if (!fullTextAnnotation || !fullTextAnnotation.pages) {
      return [];
    }

    const detectedBlocks: any[] = [];

    for (const page of fullTextAnnotation.pages) {
      if (!page.blocks) continue;

      for (const block of page.blocks) {
        if (!block.paragraphs) continue;

        for (const paragraph of block.paragraphs) {
          if (!paragraph.words) continue;

          let blockText = '';
          let minX = Infinity, minY = Infinity, maxX = -Infinity, maxY = -Infinity;

          for (const word of paragraph.words) {
            if (!word.symbols) continue;

            for (const symbol of word.symbols) {
              if (symbol.text) {
                blockText += symbol.text;
              }

              if (symbol.boundingBox && symbol.boundingBox.vertices) {
                for (const vertex of symbol.boundingBox.vertices) {
                  if (vertex.x !== undefined && vertex.y !== undefined) {
                    minX = Math.min(minX, vertex.x / scale);
                    minY = Math.min(minY, vertex.y / scale);
                    maxX = Math.max(maxX, vertex.x / scale);
                    maxY = Math.max(maxY, vertex.y / scale);
                  }
                }
              }
            }

            blockText += ' ';
          }

          if (blockText.trim() && minX !== Infinity) {
            detectedBlocks.push({
              text: blockText.trim(),
              boundingBox: {
                x: minX,
                y: minY,
                width: maxX - minX,
                height: maxY - minY
              },
              source,
              confidence: 0.9 // Default confidence for Google Vision
            });
          }
        }
      }
    }

    return detectedBlocks;
  }

  /**
   * Analyze text to determine mode and extract question
   */
  private static async analyzeTextForMode(visionText: string, model: ModelType): Promise<TextAnalysisResult> {
    // Use centralized prompt system
    const { getPrompt } = await import('../../config/prompts.js');
    const systemPrompt = getPrompt('textAnalysis.system');
    const userPrompt = getPrompt('textAnalysis.user', visionText);

    try {
      const result = await ModelProvider.callGeminiText(systemPrompt, userPrompt, model, true);
      const analysis = JSON.parse(result.content);
      
      return {
        mode: analysis.mode || 'unknown',
        questionText: analysis.questionText || '',
        confidence: analysis.confidence || 0,
        reasoning: analysis.reasoning || 'No reasoning provided',
        usageTokens: result.usageTokens || 0
      };
    } catch (error) {
      console.error('‚ùå [TEXT ANALYSIS] Failed to analyze text:', error);
      return {
        mode: 'unknown',
        questionText: '',
        confidence: 0,
        reasoning: 'Text analysis failed',
        usageTokens: 0
      };
    }
  }

  /**
   * Main method: Extract text and analyze for mode detection
   */
  static async extractTextAndAnalyze(imageData: string, model: ModelType): Promise<{
    textAnalysis: TextAnalysisResult;
    visionResult: VisionResult;
  }> {
    try {
      // Convert base64 to buffer
      const base64Data = imageData.replace(/^data:image\/[a-z]+;base64,/, '');
      const imageBuffer = Buffer.from(base64Data, 'base64');

      // Step 1: Google Vision text extraction (3-pass)
      console.log('üîç [TEXT ANALYSIS] Performing Google Vision text extraction...');
      const visionResult = await this.performRobustVisionExtraction(imageBuffer);
      console.log('‚úÖ [TEXT ANALYSIS] Google Vision extraction completed');

      // Step 2: Gemini text analysis
      console.log('üîç [TEXT ANALYSIS] Analyzing text with Gemini...');
      const textAnalysis = await this.analyzeTextForMode(visionResult.passAText, model);
      console.log('‚úÖ [TEXT ANALYSIS] Text analysis completed');

      return {
        textAnalysis,
        visionResult
      };
    } catch (error) {
      console.error('‚ùå [TEXT ANALYSIS] Failed to extract and analyze text:', error);
      throw error;
    }
  }

  /**
   * Backward compatibility method for old API
   * Maps new implementation to old ClassificationService interface
   */
  static async classifyImage(imageData: string, model: ModelType, debug: boolean = false, preProcessedImage?: string): Promise<ImageClassification> {
    try {
      // Use the new implementation
      const result = await this.extractTextAndAnalyze(imageData, model);
      
      // Map new format to old format for backward compatibility
      return {
        isQuestionOnly: result.textAnalysis.mode === 'question',
        reasoning: result.textAnalysis.reasoning,
        extractedQuestionText: result.textAnalysis.questionText,
        usageTokens: result.textAnalysis.usageTokens
      };
    } catch (error) {
      console.error('‚ùå [CLASSIFICATION] Failed to classify image:', error);
      // Return fallback response for backward compatibility
      return {
        isQuestionOnly: false,
        reasoning: 'Classification failed, defaulting to marking mode',
        extractedQuestionText: '',
        usageTokens: 0
      };
    }
  }
}
