/**
 * LLMOrchestrator
 * Non-breaking wrapper that uses existing AIMarkingService internals to produce
 * the same SimpleMarkingInstructions shape. This will be progressively
 * refactored to own prompts and parsing, but for now it centralizes the flow.
 */

import type {
  ModelType,
  ProcessedImageResult as SimpleProcessedImageResult,
  MarkingInstructions as SimpleMarkingInstructions
} from '../../types/index.js';

export interface MarkingInputs {
  imageData: string;
  model: ModelType;
  processedImage: SimpleProcessedImageResult;
  // Use broad type to avoid coupling to specific question detection shapes
  questionDetection?: any;
}

export class LLMOrchestrator {
  /**
   * Execute marking flow and return existing SimpleMarkingInstructions.
   * Uses the new modular services directly.
   */
  static async executeMarking(inputs: MarkingInputs): Promise<SimpleMarkingInstructions & { usage?: { llmTokens: number } }> {
    const { imageData: _imageData, model, processedImage, questionDetection } = inputs;

    // Log full raw OCR text (math block) prior to cleanup
    try {
      const boxes = processedImage.boundingBoxes || [];
      // OCR math blocks log removed for cleaner output
    } catch {}

    try {
      // Step 0a: Assign step_id to original OCR text first
      const { OCRCleanupService } = await import('./OCRCleanupService.js');
      
      // Transform mathBlocks to the expected format for assignStepIds
      // Handle both data structures: nested boundingBox and flat properties
      const transformedBoundingBoxes = (processedImage.boundingBoxes || []).map((block: any, index: number) => {
        // Try multiple nested structures: boundingBox, coordinates, etc.
        let x = block.boundingBox?.x;
        let y = block.boundingBox?.y;
        let width = block.boundingBox?.width;
        let height = block.boundingBox?.height;
        let text = block.boundingBox?.text;
        
        // If boundingBox structure is missing, try coordinates structure
        if (x === undefined || y === undefined || width === undefined || height === undefined) {
          x = block.coordinates?.x;
          y = block.coordinates?.y;
          width = block.coordinates?.width;
          height = block.coordinates?.height;
          text = block.coordinates?.text;
        }
        
        // If coordinates structure is missing, try flat structure (block.x, block.y, etc.)
        if (x === undefined || y === undefined || width === undefined || height === undefined) {
          x = block.x;
          y = block.y;
          width = block.width;
          height = block.height;
          text = block.text;
        }
        
        // Validate coordinates
        if (x === undefined || y === undefined || width === undefined || height === undefined) {
          console.error(`‚ùå [OCR DEBUG] Block ${index} has invalid coordinates:`, {
            x, y, width, height,
            hasBoundingBox: !!block.boundingBox,
            hasCoordinates: !!block.coordinates,
            boundingBoxKeys: block.boundingBox ? Object.keys(block.boundingBox) : [],
            coordinatesKeys: block.coordinates ? Object.keys(block.coordinates) : [],
            blockKeys: Object.keys(block)
          });
          throw new Error(`Block ${index} has invalid coordinates: x=${x}, y=${y}, width=${width}, height=${height}`);
        }
        
        // Validate coordinate values
        if (isNaN(x) || isNaN(y) || isNaN(width) || isNaN(height)) {
          console.error(`‚ùå [OCR DEBUG] Block ${index} has NaN coordinates:`, { x, y, width, height });
          throw new Error(`Block ${index} has NaN coordinates: x=${x}, y=${y}, width=${width}, height=${height}`);
        }
        
        if (x < 0 || y < 0 || width <= 0 || height <= 0) {
          console.error(`‚ùå [OCR DEBUG] Block ${index} has invalid coordinate values:`, { x, y, width, height });
          throw new Error(`Block ${index} has invalid coordinate values: x=${x}, y=${y}, width=${width}, height=${height}`);
        }
        
        return {
          x: Number(x),
          y: Number(y),
          width: Number(width),
          height: Number(height),
          text: text || block.googleVisionText || block.mathpixLatex || '',
          confidence: block.confidence || 0
        };
      });
      
      console.log('üîç [COORDINATE DEBUG] Transformed bounding boxes before assignStepIds:', transformedBoundingBoxes.map(b => ({
        x: b.x, y: b.y, width: b.width, height: b.height, text: b.text
      })));
      
      const stepAssignmentResult = await OCRCleanupService.assignStepIds(
        model,
        processedImage.ocrText || '',
        transformedBoundingBoxes
      );
      
      // Step 0b: Clean up OCR text while preserving step_id references
      const cleanupResult = await OCRCleanupService.cleanOCRTextWithStepIds(
        model,
        stepAssignmentResult.originalWithStepIds
      );
      let totalTokens = cleanupResult.usageTokens || 0;

      // Parse the step assignment result (not used in current implementation)
      try {
        JSON.parse(stepAssignmentResult.originalWithStepIds);
      } catch (error) {
        console.error('‚ùå Failed to parse step assignment JSON:', error);
      }

      // Parse the cleaned OCR to extract steps with step_id and cleaned text
      let cleanedData;
      try {
        // Use JsonUtils for robust JSON cleaning and parsing
        const { JsonUtils } = await import('./JsonUtils');
        
        if (!cleanupResult.cleanedText || cleanupResult.cleanedText.trim() === '') {
          throw new Error('OCR cleanup returned empty text');
        }
        
        cleanedData = JsonUtils.cleanAndValidateJSON(cleanupResult.cleanedText, 'steps');
        
        if (!cleanedData.steps || cleanedData.steps.length === 0) {
          console.error('‚ùå [OCR DEBUG] No steps found in cleaned data!');
          console.error('‚ùå [OCR DEBUG] Full cleaned data structure:', JSON.stringify(cleanedData, null, 2));
          throw new Error('OCR cleanup failed to extract any steps');
        }
        
      } catch (error) {
        console.error('‚ùå [OCR DEBUG] Failed to parse cleaned OCR JSON with JsonUtils:', error);
        console.error('‚ùå [OCR DEBUG] Error details:', error instanceof Error ? error.message : 'Unknown error');
        console.error('‚ùå [OCR DEBUG] Raw cleaned text (first 1000 chars):', cleanupResult.cleanedText.substring(0, 1000));
        
        // Try manual parsing as fallback
        try {
          let jsonText = cleanupResult.cleanedText
            .replace(/```json\n?/g, '')
            .replace(/```\n?/g, '')
            .trim();
          
          // Try to extract JSON from the response if it's embedded in text
          const jsonMatch = jsonText.match(/\{[\s\S]*\}/);
          if (jsonMatch) {
            jsonText = jsonMatch[0];
          }
          
          
          cleanedData = JSON.parse(jsonText);
          
          // Validate the parsed data structure
          if (!cleanedData.steps || !Array.isArray(cleanedData.steps)) {
            throw new Error('Invalid data structure: missing or invalid steps array');
          }
          
          
        } catch (manualError) {
          console.error('‚ùå Manual JSON parsing also failed:', manualError);
          
          // Final fallback: Create a single step with the raw cleaned text
          cleanedData = { 
            question: "Unknown question",
            steps: [{ 
              unified_step_id: 'step_1', 
              original_text: cleanupResult.cleanedText,
              cleaned_text: cleanupResult.cleanedText,
              bbox: [0, 0, 100, 30]
            }] 
          };
          
        }
      }

      // Step 1: Generate raw annotations from cleaned OCR text
      const { MarkingInstructionService } = await import('./MarkingInstructionService');
      
      // Create a clean copy for marking service (remove original_text to avoid confusing the AI)
      const cleanDataForMarking = {
        question: cleanedData.question || "Unknown question",
        steps: cleanedData.steps?.map((step: any) => ({
          unified_step_id: step.unified_step_id,
          bbox: step.bbox,
          cleaned_text: step.cleaned_text
        })) || []
      };
      
      if (!cleanDataForMarking.steps || cleanDataForMarking.steps.length === 0) {
        console.error('‚ùå [AI DEBUG] No steps available for annotation generation!');
        console.error('‚ùå [AI DEBUG] This will cause the AI to generate empty annotations');
        throw new Error('Cannot generate annotations without steps - OCR cleanup failed');
      }
      
      
      const annotationData = await MarkingInstructionService.generateFromOCR(
        model,
        JSON.stringify(cleanDataForMarking),
        questionDetection
      );
      totalTokens += annotationData.usageTokens || 0;
      
      console.log('üîç [ANNOTATION DEBUG] AI generated annotations count:', annotationData.annotations?.length || 0);
      
      if (annotationData.annotations && Array.isArray(annotationData.annotations) && annotationData.annotations.length > 0) {
        console.log('üîç [ANNOTATION DEBUG] All annotation texts:', annotationData.annotations.map(a => a.text));
      } else {
        console.error('‚ùå [ANNOTATION DEBUG] Invalid annotations data:', annotationData.annotations);
        throw new Error('AI failed to generate valid annotations array');
      }
      

      // Print raw cleaned OCR text from cleanup service
      

      // Step 2: Map annotations to coordinates using pre-built unified lookup table
      const { AnnotationMapper } = await import('./AnnotationMapper');
      
      // Build the complete unified lookup table from cleaned data
      const unifiedLookupTable: Record<string, { bbox: number[]; cleanedText: string }> = {};
      if (cleanedData.steps && Array.isArray(cleanedData.steps)) {
        for (const step of cleanedData.steps) {
          if (step.unified_step_id && step.bbox && Array.isArray(step.bbox) && step.bbox.length === 4) {
            unifiedLookupTable[step.unified_step_id] = {
              bbox: step.bbox,
              cleanedText: step.cleaned_text || ''
            };
          }
        }
      }
      
      const placed = await AnnotationMapper.mapAnnotations({
        ocrText: cleanupResult.cleanedText,
        boundingBoxes: transformedBoundingBoxes, // Use the same transformed bounding boxes
        rawAnnotations: annotationData,
        imageDimensions: processedImage.imageDimensions,
        unifiedLookupTable: unifiedLookupTable // Pass the complete pre-built lookup table
      });
      
      console.log('üîç [FINAL DEBUG] Mapped annotations count:', placed.annotations?.length || 0);
      if (placed.annotations?.length > 0) {
        console.log('üîç [FINAL DEBUG] Final annotation texts:', placed.annotations.map(a => a.text));
        console.log('üîç [FINAL DEBUG] First final annotation bbox:', placed.annotations[0].bbox);
      }
      

      const result: SimpleMarkingInstructions & { usage?: { llmTokens: number } } = { annotations: placed.annotations as any, usage: { llmTokens: totalTokens } };
      return result;
    } catch (error) {
      console.error('‚ùå New 2-step LLM flow failed:', error);
      console.error('‚ùå Error details:', error instanceof Error ? error.message : 'Unknown error');
      console.error('‚ùå Error stack:', error instanceof Error ? error.stack : 'No stack trace');
      
      // Throw the real error instead of failing silently
      throw new Error(`LLM marking flow failed: ${error instanceof Error ? error.message : 'Unknown error'}`);
    }
  }
}


