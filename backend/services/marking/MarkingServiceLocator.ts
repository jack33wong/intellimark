/**
 * AI Marking Service
 * Handles AI-powered homework marking with image classification and annotation generation
 */

import * as path from 'path';
import { getModelConfig } from '../../config/aiModels.js';
import { ErrorHandler } from '../../utils/errorHandler.js';


import {
  ModelType,
  ImageClassification,
  ProcessedImageResult,
  Annotation,
  MarkingInstructions
} from '../../types/index.js';
import { getPrompt } from '../../config/prompts.js';
import { validateModel } from '../../config/aiModels.js';


export class MarkingServiceLocator {
  /**
   * Classify image as question-only or question+answer
   */
  static async classifyImage(
    imageData: string,
    model: ModelType
  ): Promise<ImageClassification> {
    const { ClassificationService } = await import('./ClassificationService');
    return ClassificationService.classifyImage(imageData, model);
  }



  /**
   * Generate marking instructions for homework images
   */
  static async generateMarkingInstructions(
    imageData: string,
    model: ModelType,
    processedImage?: ProcessedImageResult,
    questionDetection?: any
  ): Promise<MarkingInstructions> {
    const { MarkingInstructionService } = await import('./MarkingInstructionService');
    return MarkingInstructionService.executeMarking({
      imageData,
      model,
      processedImage: processedImage || ({} as ProcessedImageResult),
      questionDetection
    });
  }

  /**
   * Generate contextual response for text-based conversations
   */
  static async generateContextualResponse(
    message: string,
    chatHistory: any[],
    model: ModelType,
    contextSummary?: string
  ): Promise<{ response: string; apiUsed: string; confidence: number; usageTokens: number }> {

    const systemPrompt = getPrompt('marking.contextual.system');

    // Use context summary if available, otherwise fall back to recent messages
    let contextPrompt = '';
    if (contextSummary) {
      contextPrompt = `\n\nPrevious conversation summary:\n${contextSummary}`;
    } else if (chatHistory.length > 0) {
      // Always provide context - let the AI decide what's relevant
      contextPrompt = `\n\nPrevious conversation context:\n${chatHistory.slice(-3).map(item => `${item.role}: ${item.content}`).join('\n')}`;
    }

    const userPrompt = getPrompt('marking.contextual.user', message, contextPrompt);

    try {
      const { ModelProvider } = await import('../../utils/ModelProvider.js');
      const response = await ModelProvider.callGeminiText(systemPrompt, userPrompt, 'auto');

      const { getModelInfo } = await import('../../config/aiModels.js');
      const modelInfo = getModelInfo(model);
      const apiUsed = `Google ${modelInfo.modelName} (Service Account)`;

      return {
        response: response.content,
        apiUsed: apiUsed,
        confidence: 0.95, // Default confidence for AI responses (text mode)
        usageTokens: response.usageTokens || 0
      };
    } catch (error) {
      console.error('‚ùå Contextual response generation failed:', error);
      return {
        response: 'I apologize, but I encountered an error while processing your message. Please try again.',
        apiUsed: 'Error',
        confidence: 0,
        usageTokens: 0
      };
    }
  }

  /**
   * Generate chat response for question-only images or marking feedback from OCR text
   */
  static async generateChatResponse(
    imageDataOrOcrText: string,
    message: string,
    model: ModelType,
    category: "questionOnly" | "questionAnswer" | "metadata" | "frontPage" = "questionOnly",
    debug: boolean = false,
    onProgress?: (data: any) => void,
    useOcrText: boolean = false,
    tracker?: any,  // UsageTracker (optional)
    markingScheme?: string  // NEW: Marking scheme for model answer generation
  ): Promise<{ response: string; apiUsed: string; confidence: number; usageTokens: number; inputTokens?: number; outputTokens?: number }> {

    // Debug mode: Return mock response
    if (debug) {
      return {
        response: 'Debug mode: Mock chat response - This is a simulated AI response for testing purposes.',
        apiUsed: 'Debug Mode - Mock Response',
        confidence: 0.85,
        usageTokens: 150
      };
    }

    // Handle both image and OCR text inputs
    let compressedImage: string | null = null;
    let ocrText: string | null = null;

    if (useOcrText) {
      ocrText = imageDataOrOcrText;
    } else {
      compressedImage = imageDataOrOcrText;
    }

    const isQuestionOnly = category === "questionOnly";
    const systemPrompt = isQuestionOnly
      ? getPrompt('marking.questionOnly.system')
      : getPrompt('modelAnswer.system')

    const userPrompt = isQuestionOnly
      // If no marking scheme (detection failed), use default message which instructs AI to solve without scheme
      ? getPrompt('marking.questionOnly.user', message, markingScheme || 'No marking scheme available. Solve as a mathematician.')
      : getPrompt('modelAnswer.user', ocrText, message); // ocrText and schemeJson (message)


    try {
      // Call progress callback to indicate AI response generation is starting
      if (onProgress) {
        onProgress({
          currentStepDescription: 'Generating response...',
          completedSteps: ['classification', 'question_detection'],
          allSteps: [
            { id: 'classification', description: 'Analyzing image...' },
            { id: 'question_detection', description: 'Detecting question type...' },
            { id: 'ai_response', description: 'Generating response...' }
          ],
          isComplete: false
        });
      }

      // Validate model using centralized validation
      const validatedModel = validateModel(model);

      // Both marking mode and question mode use text-only responses (no image sent to AI)
      // For Question Mode: message contains question text from database
      // For Marking Mode: ocrText contains student work
      const textInput = isQuestionOnly ? message : ocrText;

      return await this.callGeminiForTextResponse(textInput, systemPrompt, userPrompt, validatedModel, tracker, category);
    } catch (error) {
      // Check if this is our validation error (fail fast)
      if (error instanceof Error && error.message.includes('Unsupported model')) {
        // This is our validation error - re-throw it as-is
        throw error;
      }

      // This is a Google API error - log with proper context
      const { getModelInfo } = await import('../../config/aiModels.js');
      const modelInfo = getModelInfo(model);

      console.error(`‚ùå [GOOGLE API ERROR] Failed with model: ${modelInfo.modelName} (${modelInfo.apiVersion})`);
      console.error(`‚ùå [API ENDPOINT] ${modelInfo.config.apiEndpoint}`);
      console.error(`‚ùå [GOOGLE ERROR] ${error instanceof Error ? error.message : 'Unknown error'}`);

      // Use unified error handling
      const errorInfo = ErrorHandler.analyzeError(error);
      console.log(ErrorHandler.getLogMessage(error, `chat response model: ${modelInfo.modelName}`));

      // Fail fast - no fallbacks
      throw error;
    }
  }


  /**
   * Call Gemini API for chat response with image
   */
  private static async callGeminiForChatResponse(
    imageData: string,
    systemPrompt: string,
    userPrompt: string,
    model: ModelType = 'auto',
    tracker?: any
  ): Promise<{ response: string; apiUsed: string; confidence: number; usageTokens: number; inputTokens?: number; outputTokens?: number }> {
    try {
      // Check if model is OpenAI - route to OpenAI API instead
      const isOpenAI = model.toString().startsWith('openai-');

      if (isOpenAI) {
        // Use OpenAI Chat API for OpenAI models
        // Question Mode doesn't need JSON format (prompts don't mention "json")
        const { ModelProvider } = await import('../../utils/ModelProvider.js');
        const openaiModelName = model.toString().replace('openai-', '');
        const result = await ModelProvider.callOpenAIChat(systemPrompt, userPrompt, imageData, openaiModelName, false, tracker, 'marking'); // false = no JSON format

        return {
          response: result.content,
          apiUsed: `OpenAI ${result.modelName}`,
          confidence: 0.85,
          usageTokens: result.usageTokens || 0
        };
      }

      // Use Gemini API for Gemini models
      const { ModelProvider } = await import('../../utils/ModelProvider.js');
      const accessToken = ModelProvider.getGeminiApiKey();

      // [DEBUG] Log raw input prompt for Question Mode validation
      console.log(`\nüîç [QUESTION MODE PROMPT]`);
      console.log(`SYSTEM: ${systemPrompt}`);
      console.log(`USER: ${userPrompt}`);
      console.log('----------------------------------------\n');

      const response = await this.makeGeminiChatRequest(accessToken, imageData, systemPrompt, userPrompt, model);
      const result = await response.json() as any;

      // [DEBUG] Log raw JSON response for Question Mode validation
      console.log(`\nüîç [QUESTION MODE RAW RESPONSE]`);
      console.log(JSON.stringify(result, null, 2));
      console.log('----------------------------------------\n');

      const content = this.extractGeminiChatContent(result);

      const { getModelInfo } = await import('../../config/aiModels.js');
      const modelInfo = getModelInfo(model);
      const apiUsed = `Google ${modelInfo.modelName} (Service Account)`;

      return {
        response: content,
        apiUsed: apiUsed,
        confidence: 0.85, // Default confidence for AI responses (question mode)
        usageTokens: (result.usageMetadata?.totalTokenCount as number) || 0,
        inputTokens: (result.usageMetadata?.promptTokenCount as number) || 0,
        outputTokens: (result.usageMetadata?.candidatesTokenCount as number) || 0
      };
    } catch (error) {
      console.error('‚ùå Gemini chat response failed:', error);

      const errorInfo = ErrorHandler.analyzeError(error);
      if (errorInfo.isRateLimit) {
        const { getModelInfo } = await import('../../config/aiModels.js');
        const modelInfo = getModelInfo(model);
        console.error(`‚ùå [QUOTA EXCEEDED] ${modelInfo.modelName} (${modelInfo.apiVersion}) quota exceeded for chat response`);
        console.error(`‚ùå [API ENDPOINT] ${modelInfo.config.apiEndpoint}`);
        console.error(`‚ùå [ERROR DETAILS] ${error.message}`);
        throw new Error(`API quota exceeded for ${modelInfo.modelName} (${modelInfo.apiVersion}) chat response. Please check your Google Cloud Console for quota limits.`);
      }

      throw error;
    }
  }

  /**
   * Call Gemini API for text-only responses (no image)
   */
  private static async callGeminiForTextResponse(
    ocrText: string,
    systemPrompt: string,
    userPrompt: string,
    model: ModelType = 'auto',
    tracker?: any,
    category?: "questionOnly" | "questionAnswer" | "metadata" | "frontPage"
  ): Promise<{ response: string; apiUsed: string; confidence: number; usageTokens: number }> {
    try {
      // Check if model is OpenAI - route to OpenAI API instead
      const isOpenAI = model.toString().startsWith('openai-');

      if (isOpenAI) {
        // Use OpenAI Text API for OpenAI models
        const { ModelProvider } = await import('../../utils/ModelProvider.js');
        const openaiModelName = model.toString().replace('openai-', '');
        const result = await ModelProvider.callOpenAIText(systemPrompt, userPrompt, openaiModelName, false, tracker, 'marking');

        return {
          response: result.content,
          apiUsed: `OpenAI ${openaiModelName}`,
          confidence: 0.85,
          usageTokens: result.usageTokens || 0
        };
      }

      // Use Gemini API for Gemini models
      const { ModelProvider } = await import('../../utils/ModelProvider.js');

      // Determine correct phase for token tracking based on category
      const isQuestionOnly = category === "questionOnly";
      const phase = isQuestionOnly ? 'questionMode' : 'marking';

      const result = await ModelProvider.callGeminiText(systemPrompt, userPrompt, model, false, tracker, phase);

      const { getModelInfo } = await import('../../config/aiModels.js');
      const modelInfo = getModelInfo(model);
      const apiUsed = `Google ${modelInfo.modelName} (Service Account)`;

      return {
        response: result.content,
        apiUsed: apiUsed,
        confidence: 0.85, // Default confidence for AI responses (marking mode)
        usageTokens: result.usageTokens || 0
      };
    } catch (error) {
      console.error('‚ùå Gemini text response failed:', error);

      const errorInfo = ErrorHandler.analyzeError(error);
      if (errorInfo.isRateLimit) {
        const { getModelInfo } = await import('../../config/aiModels.js');
        const modelInfo = getModelInfo(model);
        console.error(`‚ùå [QUOTA EXCEEDED] ${modelInfo.modelName} (${modelInfo.apiVersion}) quota exceeded for text response`);
        console.error(`‚ùå [API ENDPOINT] ${modelInfo.config.apiEndpoint}`);
        console.error(`‚ùå [ERROR DETAILS] ${error.message}`);
        throw new Error(`API quota exceeded for ${modelInfo.modelName} (${modelInfo.apiVersion}) text response. Please check your Google Cloud Console for quota limits.`);
      }

      throw error;
    }
  }
  private static async makeGeminiChatRequest(
    accessToken: string,
    imageData: string,
    systemPrompt: string,
    userPrompt: string,
    model: ModelType = 'gemini-2.5-pro'
  ): Promise<Response> {
    const { getModelInfo } = await import('../../config/aiModels.js');
    const modelInfo = getModelInfo(model);

    const response = await fetch(`${modelInfo.config.apiEndpoint}?key=${accessToken}`, {
      method: 'POST',
      headers: {
        'Content-Type': 'application/json'
      },
      body: JSON.stringify({
        contents: [{
          parts: [
            { text: systemPrompt },
            { text: userPrompt },
            {
              inline_data: {
                mime_type: 'image/jpeg',
                data: imageData.split(',')[1]
              }
            }
          ]
        }],
        generationConfig: {
          temperature: 0.7,
          maxOutputTokens: modelInfo.config.maxTokens, // Use centralized config
        }
      })
    });

    if (!response.ok) {
      // Capture error response body for detailed diagnostics
      let errorText = '';
      try {
        errorText = await response.text();
      } catch (e) {
        errorText = 'Unable to read error response body';
      }

      console.error(`‚ùå [GEMINI CHAT API ERROR] Failed with model: ${modelInfo.modelName} (${modelInfo.apiVersion})`);
      console.error(`‚ùå [API ENDPOINT] ${modelInfo.config.apiEndpoint}`);
      console.error(`‚ùå [HTTP STATUS] ${response.status} ${response.statusText}`);
      console.error(`‚ùå [ERROR RESPONSE BODY] ${errorText}`);

      // Try to parse error body for structured error info
      let parsedError = null;
      try {
        parsedError = JSON.parse(errorText);
        if (parsedError.error) {
          console.error(`‚ùå [ERROR DETAILS]`, JSON.stringify(parsedError.error, null, 2));
        }
      } catch (e) {
        // Not JSON, that's okay
      }

      // Include error details in thrown error
      const errorMessage = parsedError?.error?.message || errorText || response.statusText;
      throw new Error(`Gemini API request failed: ${response.status} ${response.statusText} for ${modelInfo.modelName} (${modelInfo.apiVersion}) - ${errorMessage}`);
    }

    return response;
  }
  private static extractGeminiChatContent(result: any): string {
    // Check if this is an error response first
    if (result.error) {
      console.error('‚ùå [DEBUG] Gemini API returned error response:', result.error);
      throw new Error(`Gemini API error: ${result.error.message || 'Unknown error'}`);
    }

    const content = result.candidates?.[0]?.content?.parts?.[0]?.text;

    if (!content) {
      console.error('‚ùå [DEBUG] No content found in Gemini response');
      console.error('‚ùå [DEBUG] Response structure:', {
        hasCandidates: !!result.candidates,
        candidatesLength: result.candidates?.length,
        firstCandidate: result.candidates?.[0],
        hasContent: !!result.candidates?.[0]?.content,
        hasParts: !!result.candidates?.[0]?.content?.parts,
        partsLength: result.candidates?.[0]?.content?.parts?.length,
        firstPart: result.candidates?.[0]?.content?.parts?.[0]
      });

      // Check for safety filters or other issues
      const finishReason = result.candidates?.[0]?.finishReason;
      if (finishReason) {
        console.error('‚ùå [DEBUG] Finish reason:', finishReason);

        // Handle MAX_TOKENS specifically
        if (finishReason === 'MAX_TOKENS') {
          console.error('‚ùå [MAX_TOKENS] Response truncated due to token limit. Consider using a model with higher limits or reducing prompt length.');
          throw new Error(`Response truncated due to token limit. The model response was too long and got cut off. Consider using a model with higher token limits for longer responses.`);
        }
      }

      throw new Error(`No content in Gemini response. Finish reason: ${finishReason || 'unknown'}`);
    }

    return content;
  }



}
